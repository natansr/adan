 =============O MODELO BERT=============

O modelo BERT desempenha um papel importante nesse problema em questão. 

O BERT (Bidirectional Encoder Representations from Transformers) é um modelo de linguagem pré-treinado que captura informações contextuais e semânticas das palavras 
em um texto. Ele é usado para obter representações ricas e contextualizadas das palavras nos títulos, abstracts e coautores dos documentos.

No exemplo, o BERT é usado para pré-processar os dados de entrada. Cada documento é construído concatenando o título, abstract e coautor, 
formando um texto composto que representa o documento como um todo. 
Esse texto composto é então passado para o tokenizer do BERT, que o divide em tokens e adiciona os tokens especiais necessários. 
O BERT produz os IDs de entrada (input IDs) e as máscaras de atenção (attention masks) para cada documento.

Em seguida, esses IDs de entrada e máscaras de atenção são fornecidos ao modelo BERT, que retorna as saídas do BERT para cada documento. 
No exemplo, é utilizado o último estado oculto (last hidden state) correspondente ao token especial [CLS] para representar o documento 
como um vetor de características (embedding) de tamanho fixo. Esses embeddings são usados como entrada para a rede GCN.

Em resumo, o BERT é responsável por capturar as informações semânticas e contextuais dos documentos, 
permitindo que a rede GCN se beneficie dessas representações ricas para realizar a classificação dos documentos 
com base nas classes de autor. O pré-treinamento do BERT em uma grande quantidade de dados textuais permite que 
ele aprenda uma representação geral de linguagem, que é transferida e refinada no exemplo específico de classificação de documentos.

- A construção do grafo

No problema específico que estamos abordando, os nós representam os documentos, enquanto as arestas representam a relação de coautoria entre os documentos. 
Cada nó do grafo representa um documento, e uma aresta é adicionada entre dois nós se os documentos correspondentes tiverem pelo menos um coautor em comum.
Portanto, a conexão entre dois nós é estabelecida quando há uma relação de coautoria entre os documentos correspondentes. 
Isso permite capturar a relação de coautoria e a estrutura do grafo entre os documentos no problema de classificação de autor.

Nesse problema os grafos são construídos a partir da coautoria dos documentos. 
Nesse contexto, cada nó (ou vértice) do grafo representa um documento específico. 
As arestas do grafo são criadas com base na coautoria entre documentos. 
Portanto, se dois documentos compartilham pelo menos um autor, uma aresta é criada entre esses dois documentos (nós).

Essa construção de grafo reflete a estrutura de coautoria dos documentos. 
Ela captura a ideia de que documentos que compartilham autores estão de alguma forma relacionados e essa relação pode ser útil 
para tarefas como a classificação de documentos e portanto resolver o problema de AND.




- A GCN

Para começar, as Redes de Convolução Gráfica (GCN) são uma extensão das redes de convolução tradicionais para dados estruturados como grafos. 
Elas são projetadas para operar diretamente sobre grafos e extrair características de alta ordem dos nós, 
levando em consideração tanto as características dos nós quanto a estrutura do grafo.

1. Inicialização (init):

Na inicialização, definem-se duas camadas de convolução gráfica, conv1 e conv2. Essas camadas transformam as características de entrada (in_feats) 
em características de saída (out_feats), passando por características ocultas (hid_feats) na camada intermediária. 
A ideia por trás da utilização de múltiplas camadas é permitir que a rede aprenda características de ordem superior ao agregar 
informações de vizinhos de segunda ordem, terceira ordem, etc.

2. Propagação para frente (forward):

    Na propagação para frente, a função forward aceita um grafo g e um tensor de características de entrada h e realiza as seguintes operações:

a. Primeira camada de convolução gráfica: 

    Os dados passam pela primeira camada de convolução gráfica self.conv1(g, h). 
    Esta camada transforma as características de entrada em características ocultas, incorporando informações dos nós vizinhos no grafo. 
    A função de ativação ReLU é aplicada para introduzir não linearidade na rede.

b. Segunda camada de convolução gráfica: 

    As características ocultas passam pela segunda camada de convolução gráfica self.conv2(g, h).
    Esta camada transforma as características ocultas em características de saída.
    Diferentemente da primeira camada, aqui não aplicamos uma função de ativação, 
    já que essas características serão usadas para a classificação final dos nós.

Por fim, é importante notar que, ao contrário das redes de convolução tradicionais, que operam em dados estruturados regularmente (como imagens), 
as GCNs são capazes de operar em qualquer tipo de grafo - direcionado, não-direcionado, pesado, etc. - tornando-as uma ferramenta poderosa para aprender
a partir de dados de grafo.

- Em resumo

Neste algoritmo específico, o BERT é usado para criar uma representação densa (também conhecida como "embedding") do texto de cada documento, 
que inclui o título, o resumo, o autor e os coautores. 
Essas embeddings são vetores de alta dimensão que representam a informação textual dos documentos em uma forma que os modelos de aprendizado de máquina podem usar.

Esses embeddings gerados pelo BERT são usados como características dos nós (ou seja, documentos) no grafo. 
Cada nó do grafo é associado a uma embedding BERT, que captura a informação textual do documento correspondente.

O grafo, juntamente com as embeddings dos nós, é então passado para a rede GCN (Convolutional Graph Neural Network). 
A GCN usa a estrutura do grafo e as características dos nós para aprender uma representação mais informada dos nós, 
levando em conta tanto as características individuais dos nós quanto as de seus vizinhos. 
A ideia é que os documentos que estão conectados no grafo são semelhantes de alguma forma (por exemplo, eles compartilham coautores), portanto,
as informações desses documentos conectados devem ser consideradas ao aprender a representação dos nós. 
Isso é especialmente útil para tarefas como a classificação de documentos, onde as informações contextuais podem ser cruciais para fazer previsões precisas.

===================


Usar diferentes tipos de incorporações pré-treinadas: O modelo atual usa o BERT como uma incorporação pré-treinada. Você pode explorar diferentes incorporações pré-treinadas, como Word2Vec, FastText, GPT-2, GPT-3, RoBERTa, etc.

Explorar diferentes arquiteturas de rede neural: No modelo atual, é usado uma Rede Convolucional de Grafos Heterogêneos (HGCN). Você pode experimentar outras arquiteturas, como a Graph Attention Network (GAT), GraphSAGE, Graph Isomorphism Network (GIN), etc.

Diferentes estratégias de caminhada aleatória: O modelo proposto usa caminhadas aleatórias guiadas por meta-caminho e peso de relação. Você pode experimentar estratégias de caminhada aleatória diferentes ou otimizar a atual.

Métodos de otimização diferentes: No modelo atual, é usado um otimizador Adam em mini-lotes. Você pode tentar usar outros otimizadores, como SGD, RMSProp, Adagrad, etc.

Modelo de aprendizado supervisionado ou semi-supervisionado: No modelo atual, o aprendizado não supervisionado é usado. Dependendo dos dados disponíveis, você pode adicionar um componente de aprendizado supervisionado ou semi-supervisionado ao seu modelo.

Incorporação de outras informações: Se o seu conjunto de dados permitir, você pode tentar incorporar outras informações além do texto, como metadados do autor, informações de citação, informações de co-autoria, etc.